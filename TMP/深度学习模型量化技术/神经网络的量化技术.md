# 神经网络的量化技术



Quantized Neural Networks:  Training Neural Networks withLow Precision Weights and Activations

http://www.jmlr.org/papers/volume18/16-456/16-456.pdf



**Quantization and Training of Neural Networks for EfficientInteger-Arithmetic-Only Inference**

> TODO 仔细阅读论文

http://openaccess.thecvf.com/content_cvpr_2018/papers/Jacob_Quantization_and_Training_CVPR_2018_paper.pdf



A Survey on Methods and Theories of Quantized Neural Networks

https://arxiv.org/pdf/1808.04752.pdf



[8-Bit Quantization and TensorFlow Lite: Speeding up mobile inference with low precision](https://heartbeat.fritz.ai/8-bit-quantization-and-tensorflow-lite-speeding-up-mobile-inference-with-low-precision-a882dfcafbbd)

https://sahnimanas.github.io/2018/06/24/quantization-in-tf-lite.html



通俗易懂

浮点数-> 固定点数。

8bit固定点数对最终结果的影响很小。



![](https://sahnimanas.github.io/assets/img/quantization/floatfixed.png)





[Pete Warden's blog](https://petewarden.com/) 系列博客

Why are Eight Bits Enough for Deep Neural Networks?

https://petewarden.com/2015/05/23/why-are-eight-bits-enough-for-deep-neural-networks/



How to Quantize Neural Networks with TensorFlow

https://petewarden.com/2016/05/03/how-to-quantize-neural-networks-with-tensorflow/



What I’ve learned about neural network quantization

https://petewarden.com/2017/06/22/what-ive-learned-about-neural-network-quantization/